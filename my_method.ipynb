{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a656df4a",
   "metadata": {},
   "source": [
    "# Datasets to test on\n",
    "- WMT en-fr\n",
    "- WMT en-gr\n",
    "- WikiText-103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0fb4894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (2.9.1)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.3.3)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (2.4.1)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (3.10.8)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (4.57.6)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (4.5.0)\n",
      "Requirement already satisfied: evaluate in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (0.4.6)\n",
      "Requirement already satisfied: nltk in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (3.9.2)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (1.8.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (2025.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 2)) (2025.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (3.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 5)) (0.36.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 5)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 5)) (2026.1.15)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 5)) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 5)) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 5)) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 5)) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 6)) (23.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 6)) (0.4.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 6)) (0.28.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 6)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 6)) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (3.13.3)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 6)) (4.12.1)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 6)) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 6)) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 6)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets->-r requirements.txt (line 6)) (0.16.0)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 8)) (8.3.1)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 8)) (1.5.3)\n",
      "Requirement already satisfied: scipy>=1.10.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 9)) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 9)) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers->-r requirements.txt (line 5)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers->-r requirements.txt (line 5)) (2.6.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "858935be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tokenizers import Tokenizer\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc78dba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreeyansarora/Downloads/Polar_Representation_Attention/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': 'Resumption of the session', 'fr': 'Reprise de la session'}}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('wmt14', 'fr-en')\n",
    "\n",
    "train_data = dataset['train'].select(range(1000000))\n",
    "test_data = dataset['test']\n",
    "print(train_data[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7badb2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(dataset, src=\"en\", tgt=\"fr\"):\n",
    "    for example in dataset:\n",
    "        yield example[\"translation\"][src]\n",
    "        yield example[\"translation\"][tgt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ac7779d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=32000,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
    ")\n",
    "\n",
    "tokenizer.train_from_iterator(\n",
    "    extract_text(train_data),\n",
    "    trainer=trainer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d9a5560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import numpy as np\n",
    "\n",
    "def decode_to_tokens(ids_list, tokenizer):\n",
    "    \"\"\"\n",
    "    Converts a list of token IDs into a list of string tokens,\n",
    "    skipping special tokens like [PAD], [BOS], [EOS].\n",
    "    \"\"\"\n",
    "    # .decode() handles the skipping of special tokens automatically\n",
    "    # .split() turns the sentence string into a list of tokens for BLEU\n",
    "    decoded_text = tokenizer.decode(ids_list, skip_special_tokens=True)\n",
    "    return decoded_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c635571f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.save(\"wmt14_bpe.json\")\n",
    "PAD_ID = tokenizer.token_to_id(\"[PAD]\")\n",
    "BOS_ID = tokenizer.token_to_id(\"[BOS]\")\n",
    "EOS_ID = tokenizer.token_to_id(\"[EOS]\")\n",
    "\n",
    "def tokenize_sentence(sentence, add_special_tokens=True):\n",
    "    encoding = tokenizer.encode(sentence)\n",
    "    token_ids = encoding.ids\n",
    "\n",
    "    if add_special_tokens:\n",
    "        token_ids = [BOS_ID] + token_ids + [EOS_ID]\n",
    "\n",
    "    return token_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74d374cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: list of dicts from wmt14 \n",
    "    Example item: {'translation': {'en': 'Hello', 'fr': 'Bonjour'}}\n",
    "    \"\"\"\n",
    "    src_list = []\n",
    "    tgt_list = []\n",
    "    \n",
    "    for item in batch:\n",
    "        en_text = item['translation']['en']\n",
    "        fr_text = item['translation']['fr']\n",
    "        \n",
    "        src_ids = tokenize_sentence(en_text)[:128] \n",
    "        tgt_ids = tokenize_sentence(fr_text)[:128]\n",
    "        \n",
    "        src_list.append(torch.tensor(src_ids, dtype=torch.long))\n",
    "        tgt_list.append(torch.tensor(tgt_ids, dtype=torch.long))\n",
    "\n",
    "    src_padded = pad_sequence(\n",
    "        src_list,\n",
    "        batch_first=True,\n",
    "        padding_value=PAD_ID\n",
    "    )\n",
    "\n",
    "    tgt_padded = pad_sequence(\n",
    "        tgt_list,\n",
    "        batch_first=True,\n",
    "        padding_value=PAD_ID\n",
    "    )\n",
    "\n",
    "    return src_padded, tgt_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f89067ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e83b53c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33fe997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def generate_compact_dataset(num_samples=10000):\n",
    "#    data_tokens = torch.arange(1, 11) \n",
    "#    \n",
    "#    all_inputs = []\n",
    "#    all_targets = []\n",
    "#\n",
    "#    for _ in range(num_samples):\n",
    "#        perm = torch.randperm(10)\n",
    "#        sample_data = data_tokens[perm[:6]]\n",
    "#        \n",
    "#        is_relational = torch.rand(1) > 0.5\n",
    "#        \n",
    "#        if is_relational:\n",
    "#            cmd = torch.tensor([12])\n",
    "#            # Pick a key from the first 5 (so there is a neighbor at +1)\n",
    "#            key_idx = torch.randint(0, 5, (1,)).item()\n",
    "#            query = sample_data[key_idx].view(1)\n",
    "#            target = sample_data[key_idx + 1]\n",
    "#        else:\n",
    "#            # POSITIONAL: Input[7] is an Index (1-6); Target is data at that index\n",
    "#            cmd = torch.tensor([11])\n",
    "#            idx_to_pull = torch.randint(0, 6, (1,)).item()\n",
    "#            query = torch.tensor([idx_to_pull + 1])\n",
    "#            target = sample_data[idx_to_pull]\n",
    "#\n",
    "#        full_input = torch.cat([sample_data, cmd, query])\n",
    "#        \n",
    "#        all_inputs.append(full_input)\n",
    "#        all_targets.append(target)\n",
    "#\n",
    "#    return torch.stack(all_inputs), torch.stack(all_targets)\n",
    "#\n",
    "## Generate the 10,000 samples\n",
    "#inputs, targets = generate_compact_dataset(10000)\n",
    "#\n",
    "#print(f\"Dataset Shape: {inputs.shape}\") # [10000, 8]\n",
    "#print(f\"Sample 0 (Input): {inputs[0].tolist()} -> Target: {targets[0].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf638b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices = torch.randperm(len(inputs))\n",
    "#\n",
    "#train_size = int(0.5*len(inputs))\n",
    "#\n",
    "#train_idx = indices[:train_size]\n",
    "#test_idx = indices[train_size:]\n",
    "#\n",
    "#train_inputs, train_targets = inputs[train_idx], targets[train_idx]\n",
    "#test_inputs,  test_targets  = inputs[test_idx],  targets[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0818ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pe(seq_len, d_model):\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    position = torch.arange(0, seq_len).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(1000000.0) / d_model))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0abdac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class embeddings(nn.Module):\n",
    "    def __init__(self, d, vocab_size=32000, max_len=512):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d)\n",
    "        self.register_buffer('pe', get_pe(max_len, d))\n",
    "\n",
    "    def forward(self, x):\n",
    "        te = self.token_emb(x)\n",
    "        return te + self.pe[:, :x.size(1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65ef7b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class relative_matrix(nn.Module):\n",
    "    def __init__(self, e_dim: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(e_dim, e_dim)\n",
    "        self.bias = nn.Parameter(torch.zeros(e_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        rel = x.unsqueeze(2) - x.unsqueeze(1) \n",
    "\n",
    "        out = rel.sum(dim=2) + self.bias\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92e78e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention_matrix(nn.Module):\n",
    "    def __init__(self, e_dim):\n",
    "        super().__init__()\n",
    "        self.e_dim = e_dim\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return torch.softmax((x @ y.transpose(-2, -1))/math.sqrt(self.e_dim), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "480edfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer(nn.Module):\n",
    "    def __init__(self, e_dim: int, vocab_size=32000):\n",
    "        super().__init__()\n",
    "        self.embeddings = embeddings(e_dim, vocab_size)\n",
    "        self.relative = relative_matrix(e_dim)\n",
    "        self.attention = attention_matrix(e_dim)\n",
    "        self.V = nn.Linear(e_dim, e_dim)\n",
    "        self.norm1 = nn.LayerNorm(e_dim)\n",
    "        self.norm2 = nn.LayerNorm(e_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(e_dim, e_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(e_dim * 4, e_dim)\n",
    "        )\n",
    "        self.e_dim = e_dim\n",
    "        self.unembed = nn.Linear(e_dim, vocab_size, bias=False)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_emb = self.embeddings(x) * math.sqrt(self.e_dim)\n",
    "        rel_key = self.relative(x_emb)\n",
    "        value = self.V(x_emb)\n",
    "        attn_map = self.attention(x_emb, rel_key) \n",
    "        attn_out = attn_map @ value\n",
    "        x = self.norm1(x_emb + self.dropout(attn_out))\n",
    "        mlp_out = self.mlp(x)\n",
    "        x = self.norm2(x + self.dropout(mlp_out))\n",
    "        logits = self.unembed(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1873ff9",
   "metadata": {},
   "source": [
    "# Test without scheduled LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67c5cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = transformer(512).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1, ignore_index=PAD_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "706868f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def noam_lambda(step, d_model=512, warmup=4000):\n",
    "#    # The scheduler passes the current_step (starting at 0)\n",
    "#    # Adding 1 prevents division by zero\n",
    "#    step += 1 \n",
    "#    return (d_model ** -0.5) * min(step ** -0.5, step * (warmup ** -1.5))\n",
    "\n",
    "# Use 1.0 as the base LR so the lambda controls the absolute value\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.98), eps=1e-9, weight_decay = 0.01)\n",
    "#scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=noam_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c1ecc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreeyansarora/Downloads/Polar_Representation_Attention/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Step:    1000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.0928 | Train Loss: 6.9658 |\n",
      "| Test Acc:  0.1076 | Test Loss:  7.6481 |\n",
      "| Test F1:   0.0725 | Test BLEU:  0.0000 |\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreeyansarora/Downloads/Polar_Representation_Attention/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Step:    2000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1362 | Train Loss: 6.8205 |\n",
      "| Test Acc:  0.1158 | Test Loss:  7.6387 |\n",
      "| Test F1:   0.0726 | Test BLEU:  0.0000 |\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreeyansarora/Downloads/Polar_Representation_Attention/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Step:    3000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1163 | Train Loss: 6.7675 |\n",
      "| Test Acc:  0.1150 | Test Loss:  7.5443 |\n",
      "| Test F1:   0.0732 | Test BLEU:  0.0000 |\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreeyansarora/Downloads/Polar_Representation_Attention/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Step:    4000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1149 | Train Loss: 6.7400 |\n",
      "| Test Acc:  0.1185 | Test Loss:  7.4861 |\n",
      "| Test F1:   0.0602 | Test BLEU:  0.0000 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:    5000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1304 | Train Loss: 6.4409 |\n",
      "| Test Acc:  0.1232 | Test Loss:  7.4052 |\n",
      "| Test F1:   0.0782 | Test BLEU:  0.0006 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:    6000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1142 | Train Loss: 6.4474 |\n",
      "| Test Acc:  0.1245 | Test Loss:  7.3393 |\n",
      "| Test F1:   0.0829 | Test BLEU:  0.0007 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:    7000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1621 | Train Loss: 6.1763 |\n",
      "| Test Acc:  0.1355 | Test Loss:  7.2472 |\n",
      "| Test F1:   0.0906 | Test BLEU:  0.0007 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:    8000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.2011 | Train Loss: 5.9882 |\n",
      "| Test Acc:  0.1394 | Test Loss:  7.2089 |\n",
      "| Test F1:   0.0930 | Test BLEU:  0.0009 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:    9000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1983 | Train Loss: 5.6883 |\n",
      "| Test Acc:  0.1484 | Test Loss:  7.1382 |\n",
      "| Test F1:   0.1003 | Test BLEU:  0.0008 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   10000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1603 | Train Loss: 5.8432 |\n",
      "| Test Acc:  0.1443 | Test Loss:  7.1526 |\n",
      "| Test F1:   0.0992 | Test BLEU:  0.0011 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   11000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1834 | Train Loss: 5.8731 |\n",
      "| Test Acc:  0.1498 | Test Loss:  7.1593 |\n",
      "| Test F1:   0.0997 | Test BLEU:  0.0010 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   12000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1469 | Train Loss: 5.8784 |\n",
      "| Test Acc:  0.1414 | Test Loss:  7.1174 |\n",
      "| Test F1:   0.1006 | Test BLEU:  0.0012 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   13000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1778 | Train Loss: 5.8275 |\n",
      "| Test Acc:  0.1399 | Test Loss:  7.1203 |\n",
      "| Test F1:   0.0988 | Test BLEU:  0.0011 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   14000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1530 | Train Loss: 6.1099 |\n",
      "| Test Acc:  0.1484 | Test Loss:  7.0964 |\n",
      "| Test F1:   0.0996 | Test BLEU:  0.0012 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   15000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.2000 | Train Loss: 5.7414 |\n",
      "| Test Acc:  0.1437 | Test Loss:  7.0822 |\n",
      "| Test F1:   0.1037 | Test BLEU:  0.0012 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   16000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.2018 | Train Loss: 5.7611 |\n",
      "| Test Acc:  0.1507 | Test Loss:  7.0573 |\n",
      "| Test F1:   0.1046 | Test BLEU:  0.0011 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   17000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1732 | Train Loss: 6.1433 |\n",
      "| Test Acc:  0.1472 | Test Loss:  7.0748 |\n",
      "| Test F1:   0.1060 | Test BLEU:  0.0014 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   18000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1924 | Train Loss: 5.8258 |\n",
      "| Test Acc:  0.1462 | Test Loss:  7.0867 |\n",
      "| Test F1:   0.1044 | Test BLEU:  0.0010 |\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreeyansarora/Downloads/Polar_Representation_Attention/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Step:   19000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1738 | Train Loss: 5.9245 |\n",
      "| Test Acc:  0.1499 | Test Loss:  7.0895 |\n",
      "| Test F1:   0.1036 | Test BLEU:  0.0000 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   20000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1621 | Train Loss: 6.0374 |\n",
      "| Test Acc:  0.1475 | Test Loss:  7.0787 |\n",
      "| Test F1:   0.1029 | Test BLEU:  0.0014 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   21000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1545 | Train Loss: 5.8794 |\n",
      "| Test Acc:  0.1502 | Test Loss:  7.0587 |\n",
      "| Test F1:   0.1015 | Test BLEU:  0.0011 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   22000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1851 | Train Loss: 5.8509 |\n",
      "| Test Acc:  0.1508 | Test Loss:  7.0950 |\n",
      "| Test F1:   0.1010 | Test BLEU:  0.0010 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   23000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1941 | Train Loss: 5.8451 |\n",
      "| Test Acc:  0.1526 | Test Loss:  7.0639 |\n",
      "| Test F1:   0.1031 | Test BLEU:  0.0013 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   24000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1913 | Train Loss: 5.6451 |\n",
      "| Test Acc:  0.1453 | Test Loss:  7.0892 |\n",
      "| Test F1:   0.1017 | Test BLEU:  0.0010 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   25000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.2024 | Train Loss: 5.7328 |\n",
      "| Test Acc:  0.1464 | Test Loss:  7.0616 |\n",
      "| Test F1:   0.1030 | Test BLEU:  0.0010 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   26000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1885 | Train Loss: 5.9958 |\n",
      "| Test Acc:  0.1486 | Test Loss:  7.0797 |\n",
      "| Test F1:   0.1006 | Test BLEU:  0.0007 |\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreeyansarora/Downloads/Polar_Representation_Attention/.venv/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Step:   27000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1805 | Train Loss: 6.0552 |\n",
      "| Test Acc:  0.1527 | Test Loss:  7.0344 |\n",
      "| Test F1:   0.1036 | Test BLEU:  0.0000 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   28000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1850 | Train Loss: 5.9360 |\n",
      "| Test Acc:  0.1536 | Test Loss:  7.0551 |\n",
      "| Test F1:   0.1041 | Test BLEU:  0.0012 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   29000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1760 | Train Loss: 5.9262 |\n",
      "| Test Acc:  0.1530 | Test Loss:  7.0852 |\n",
      "| Test F1:   0.1049 | Test BLEU:  0.0008 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   30000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1898 | Train Loss: 5.8565 |\n",
      "| Test Acc:  0.1532 | Test Loss:  7.0356 |\n",
      "| Test F1:   0.1018 | Test BLEU:  0.0009 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   31000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1702 | Train Loss: 5.8124 |\n",
      "| Test Acc:  0.1510 | Test Loss:  7.0497 |\n",
      "| Test F1:   0.1031 | Test BLEU:  0.0011 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   32000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1962 | Train Loss: 5.7060 |\n",
      "| Test Acc:  0.1470 | Test Loss:  7.0733 |\n",
      "| Test F1:   0.1030 | Test BLEU:  0.0012 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   33000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1954 | Train Loss: 5.8597 |\n",
      "| Test Acc:  0.1498 | Test Loss:  7.0444 |\n",
      "| Test F1:   0.1021 | Test BLEU:  0.0013 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   34000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1638 | Train Loss: 5.8567 |\n",
      "| Test Acc:  0.1438 | Test Loss:  7.1354 |\n",
      "| Test F1:   0.1002 | Test BLEU:  0.0012 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   35000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1906 | Train Loss: 5.8589 |\n",
      "| Test Acc:  0.1426 | Test Loss:  7.1458 |\n",
      "| Test F1:   0.1005 | Test BLEU:  0.0009 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   36000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1837 | Train Loss: 5.9131 |\n",
      "| Test Acc:  0.1531 | Test Loss:  7.0376 |\n",
      "| Test F1:   0.1030 | Test BLEU:  0.0011 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   37000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.2111 | Train Loss: 5.5579 |\n",
      "| Test Acc:  0.1474 | Test Loss:  7.0859 |\n",
      "| Test F1:   0.0996 | Test BLEU:  0.0011 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   38000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1611 | Train Loss: 5.8455 |\n",
      "| Test Acc:  0.1518 | Test Loss:  7.0283 |\n",
      "| Test F1:   0.1029 | Test BLEU:  0.0010 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   39000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1438 | Train Loss: 6.0207 |\n",
      "| Test Acc:  0.1536 | Test Loss:  7.0329 |\n",
      "| Test F1:   0.1045 | Test BLEU:  0.0012 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   40000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1605 | Train Loss: 6.2813 |\n",
      "| Test Acc:  0.1496 | Test Loss:  7.0347 |\n",
      "| Test F1:   0.1027 | Test BLEU:  0.0009 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   41000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1684 | Train Loss: 6.0428 |\n",
      "| Test Acc:  0.1497 | Test Loss:  7.0495 |\n",
      "| Test F1:   0.1015 | Test BLEU:  0.0014 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   42000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.2105 | Train Loss: 5.6047 |\n",
      "| Test Acc:  0.1459 | Test Loss:  7.0684 |\n",
      "| Test F1:   0.1006 | Test BLEU:  0.0010 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   43000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1929 | Train Loss: 5.8785 |\n",
      "| Test Acc:  0.1507 | Test Loss:  7.0339 |\n",
      "| Test F1:   0.1016 | Test BLEU:  0.0012 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   44000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1827 | Train Loss: 5.8632 |\n",
      "| Test Acc:  0.1535 | Test Loss:  7.0304 |\n",
      "| Test F1:   0.1021 | Test BLEU:  0.0014 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   45000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1573 | Train Loss: 5.8421 |\n",
      "| Test Acc:  0.1485 | Test Loss:  7.0511 |\n",
      "| Test F1:   0.1064 | Test BLEU:  0.0012 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   46000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1616 | Train Loss: 5.8408 |\n",
      "| Test Acc:  0.1556 | Test Loss:  7.0361 |\n",
      "| Test F1:   0.1059 | Test BLEU:  0.0013 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   47000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1984 | Train Loss: 5.7793 |\n",
      "| Test Acc:  0.1539 | Test Loss:  7.0538 |\n",
      "| Test F1:   0.1058 | Test BLEU:  0.0010 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   48000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1891 | Train Loss: 5.9644 |\n",
      "| Test Acc:  0.1545 | Test Loss:  7.0435 |\n",
      "| Test F1:   0.1017 | Test BLEU:  0.0014 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   49000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.2013 | Train Loss: 6.0452 |\n",
      "| Test Acc:  0.1459 | Test Loss:  7.0474 |\n",
      "| Test F1:   0.1019 | Test BLEU:  0.0010 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   50000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.2092 | Train Loss: 5.7183 |\n",
      "| Test Acc:  0.1527 | Test Loss:  7.0419 |\n",
      "| Test F1:   0.1018 | Test BLEU:  0.0012 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   51000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1927 | Train Loss: 5.9171 |\n",
      "| Test Acc:  0.1509 | Test Loss:  7.0588 |\n",
      "| Test F1:   0.1028 | Test BLEU:  0.0012 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   52000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.2040 | Train Loss: 5.7571 |\n",
      "| Test Acc:  0.1541 | Test Loss:  7.0341 |\n",
      "| Test F1:   0.1066 | Test BLEU:  0.0016 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   53000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1812 | Train Loss: 5.6230 |\n",
      "| Test Acc:  0.1520 | Test Loss:  7.0540 |\n",
      "| Test F1:   0.1039 | Test BLEU:  0.0013 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   54000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.2054 | Train Loss: 5.8879 |\n",
      "| Test Acc:  0.1532 | Test Loss:  7.0343 |\n",
      "| Test F1:   0.1054 | Test BLEU:  0.0014 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   55000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.2105 | Train Loss: 5.6406 |\n",
      "| Test Acc:  0.1518 | Test Loss:  7.0594 |\n",
      "| Test F1:   0.1028 | Test BLEU:  0.0012 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   56000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1800 | Train Loss: 5.9617 |\n",
      "| Test Acc:  0.1480 | Test Loss:  7.0788 |\n",
      "| Test F1:   0.1010 | Test BLEU:  0.0012 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   57000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1577 | Train Loss: 6.0578 |\n",
      "| Test Acc:  0.1465 | Test Loss:  7.0612 |\n",
      "| Test F1:   0.0990 | Test BLEU:  0.0012 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   58000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1985 | Train Loss: 5.7053 |\n",
      "| Test Acc:  0.1515 | Test Loss:  7.0372 |\n",
      "| Test F1:   0.1011 | Test BLEU:  0.0012 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   59000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1491 | Train Loss: 6.0330 |\n",
      "| Test Acc:  0.1456 | Test Loss:  7.0582 |\n",
      "| Test F1:   0.1022 | Test BLEU:  0.0010 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   60000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1429 | Train Loss: 6.0800 |\n",
      "| Test Acc:  0.1446 | Test Loss:  7.0666 |\n",
      "| Test F1:   0.1014 | Test BLEU:  0.0014 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   61000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1652 | Train Loss: 6.0701 |\n",
      "| Test Acc:  0.1489 | Test Loss:  7.0580 |\n",
      "| Test F1:   0.1014 | Test BLEU:  0.0011 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   62000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1778 | Train Loss: 5.9882 |\n",
      "| Test Acc:  0.1460 | Test Loss:  7.0720 |\n",
      "| Test F1:   0.1017 | Test BLEU:  0.0012 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   63000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.2020 | Train Loss: 5.8307 |\n",
      "| Test Acc:  0.1497 | Test Loss:  7.0653 |\n",
      "| Test F1:   0.1023 | Test BLEU:  0.0015 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   64000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.2024 | Train Loss: 5.8343 |\n",
      "| Test Acc:  0.1433 | Test Loss:  7.0572 |\n",
      "| Test F1:   0.0998 | Test BLEU:  0.0012 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   65000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1623 | Train Loss: 6.0971 |\n",
      "| Test Acc:  0.1474 | Test Loss:  7.0368 |\n",
      "| Test F1:   0.0995 | Test BLEU:  0.0009 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   66000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1795 | Train Loss: 5.6775 |\n",
      "| Test Acc:  0.1485 | Test Loss:  7.0575 |\n",
      "| Test F1:   0.1014 | Test BLEU:  0.0011 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   67000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1813 | Train Loss: 5.9760 |\n",
      "| Test Acc:  0.1469 | Test Loss:  7.0757 |\n",
      "| Test F1:   0.1019 | Test BLEU:  0.0010 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   68000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1924 | Train Loss: 5.4865 |\n",
      "| Test Acc:  0.1481 | Test Loss:  7.0321 |\n",
      "| Test F1:   0.0976 | Test BLEU:  0.0010 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   69000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1889 | Train Loss: 5.8755 |\n",
      "| Test Acc:  0.1458 | Test Loss:  7.0395 |\n",
      "| Test F1:   0.0997 | Test BLEU:  0.0012 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   70000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1830 | Train Loss: 5.9784 |\n",
      "| Test Acc:  0.1448 | Test Loss:  7.0477 |\n",
      "| Test F1:   0.0979 | Test BLEU:  0.0010 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   71000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.2025 | Train Loss: 5.7145 |\n",
      "| Test Acc:  0.1528 | Test Loss:  7.0312 |\n",
      "| Test F1:   0.1021 | Test BLEU:  0.0012 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   72000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1689 | Train Loss: 6.0958 |\n",
      "| Test Acc:  0.1460 | Test Loss:  7.0535 |\n",
      "| Test F1:   0.1043 | Test BLEU:  0.0011 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   73000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.2135 | Train Loss: 5.6289 |\n",
      "| Test Acc:  0.1456 | Test Loss:  7.0931 |\n",
      "| Test F1:   0.0983 | Test BLEU:  0.0009 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   74000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1879 | Train Loss: 5.8310 |\n",
      "| Test Acc:  0.1484 | Test Loss:  7.0453 |\n",
      "| Test F1:   0.0974 | Test BLEU:  0.0011 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   75000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1873 | Train Loss: 5.7272 |\n",
      "| Test Acc:  0.1470 | Test Loss:  7.0803 |\n",
      "| Test F1:   0.0994 | Test BLEU:  0.0011 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   76000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1821 | Train Loss: 5.8062 |\n",
      "| Test Acc:  0.1498 | Test Loss:  7.0487 |\n",
      "| Test F1:   0.1032 | Test BLEU:  0.0014 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   77000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1824 | Train Loss: 5.8997 |\n",
      "| Test Acc:  0.1535 | Test Loss:  7.0310 |\n",
      "| Test F1:   0.1050 | Test BLEU:  0.0010 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   78000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.1883 | Train Loss: 5.6802 |\n",
      "| Test Acc:  0.1538 | Test Loss:  7.0548 |\n",
      "| Test F1:   0.1029 | Test BLEU:  0.0011 |\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.to(DEVICE)\n",
    "global_step = 0\n",
    "log_interval = 1000\n",
    "accumulation_steps = 4 \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad() \n",
    "    \n",
    "    for i, (src, tgt) in enumerate(train_loader):\n",
    "        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "        \n",
    "        decoder_input  = tgt[:, :-1]  \n",
    "        decoder_target = tgt[:, 1:] \n",
    "        logits = model(decoder_input) \n",
    "\n",
    "        # 1. Standard Loss Calculation\n",
    "        loss = criterion(\n",
    "            logits.reshape(-1, logits.size(-1)),\n",
    "            decoder_target.reshape(-1)\n",
    "        )\n",
    "        \n",
    "        # Scale loss so gradients are averaged across the full effective batch\n",
    "        loss_scaled = loss / accumulation_steps \n",
    "        loss_scaled.backward()\n",
    "\n",
    "        # Train Accuracy (for the current mini-batch)\n",
    "        with torch.no_grad():\n",
    "            pred_tokens = logits.argmax(-1)\n",
    "            mask = decoder_target != PAD_ID\n",
    "            train_acc = ((pred_tokens == decoder_target) & mask).float().sum() / mask.sum()\n",
    "\n",
    "        # 2. Only update weights and LOG every 'accumulation_steps'\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            # scheduler.step() # Uncomment if using Noam\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            # --- EVALUATION BLOCK: Now strictly inside the update check ---\n",
    "            if global_step % log_interval == 0:\n",
    "                model.eval()\n",
    "                total_test_loss = 0\n",
    "                all_preds_ids, all_trues_ids = [], []\n",
    "                bleu_hypotheses, bleu_references = [], []\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for test_src, test_tgt in test_loader:\n",
    "                        test_src, test_tgt = test_src.to(DEVICE), test_tgt.to(DEVICE)\n",
    "                        dec_in_test, dec_tgt_test = test_tgt[:, :-1], test_tgt[:, 1:]\n",
    "                        \n",
    "                        test_logits = model(dec_in_test)\n",
    "                        t_loss = criterion(test_logits.reshape(-1, test_logits.size(-1)), dec_tgt_test.reshape(-1))\n",
    "                        total_test_loss += t_loss.item()\n",
    "\n",
    "                        test_pred_ids = test_logits.argmax(-1)\n",
    "                        test_mask = (dec_tgt_test != PAD_ID)\n",
    "                        \n",
    "                        all_preds_ids.extend(test_pred_ids[test_mask].cpu().numpy())\n",
    "                        all_trues_ids.extend(dec_tgt_test[test_mask].cpu().numpy())\n",
    "\n",
    "                        for b in range(test_pred_ids.size(0)):\n",
    "                            hyp_tokens = decode_to_tokens(test_pred_ids[b].tolist(), tokenizer)\n",
    "                            ref_tokens = decode_to_tokens(dec_tgt_test[b].tolist(), tokenizer)\n",
    "                            bleu_hypotheses.append(hyp_tokens)\n",
    "                            bleu_references.append([ref_tokens])\n",
    "\n",
    "                # Metric Calculations\n",
    "                avg_test_loss = total_test_loss / len(test_loader)\n",
    "                test_acc = (np.array(all_preds_ids) == np.array(all_trues_ids)).mean()\n",
    "                test_f1 = f1_score(all_trues_ids, all_preds_ids, average='weighted', zero_division=0)\n",
    "                test_bleu = corpus_bleu(bleu_references, bleu_hypotheses)\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                \n",
    "                output_str = (\n",
    "                    f\"| Step: {global_step:7d} | Epoch: {epoch:3d} | LR: {current_lr:.8f} |\\n\"\n",
    "                    f\"| Train Acc: {train_acc.item():.4f} | Train Loss: {loss.item():.4f} |\\n\"\n",
    "                    f\"| Test Acc:  {test_acc:.4f} | Test Loss:  {avg_test_loss:.4f} |\\n\"\n",
    "                    f\"| Test F1:   {test_f1:.4f} | Test BLEU:  {test_bleu:.4f} |\\n\"\n",
    "                    f\"{'-'*75}\"\n",
    "                )\n",
    "                print(output_str)\n",
    "                with open(\"step_results_logging.log\", \"a\") as f:\n",
    "                    f.write(output_str + \"\\n\")\n",
    "                \n",
    "                model.train() # Switch back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3346c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
