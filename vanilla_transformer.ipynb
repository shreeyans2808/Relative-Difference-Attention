{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8171d754",
   "metadata": {},
   "source": [
    "## Replication of Grokking experiments \n",
    "(checking the relation between attention logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e45ea3f",
   "metadata": {},
   "source": [
    "# Datasets to test on\n",
    "- WMT en-fr\n",
    "- WMT en-gr\n",
    "- WikiText-103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa6854ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (2.9.1)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.3.3)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (2.4.1)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (3.10.8)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (4.57.6)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (4.5.0)\n",
      "Requirement already satisfied: evaluate in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (0.4.6)\n",
      "Requirement already satisfied: nltk in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (3.9.2)\n",
      "Collecting scikit-learn (from -r requirements.txt (line 9))\n",
      "  Using cached scikit_learn-1.8.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (2025.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 2)) (2025.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 4)) (3.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 5)) (0.36.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 5)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 5)) (2026.1.15)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 5)) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 5)) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 5)) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 5)) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 6)) (23.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 6)) (0.4.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 6)) (0.28.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 6)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 6)) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (3.13.3)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 6)) (4.12.1)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 6)) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 6)) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 6)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets->-r requirements.txt (line 6)) (0.16.0)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 8)) (8.3.1)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 8)) (1.5.3)\n",
      "Collecting scipy>=1.10.0 (from scikit-learn->-r requirements.txt (line 9))\n",
      "  Using cached scipy-1.17.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn->-r requirements.txt (line 9))\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 6)) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers->-r requirements.txt (line 5)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers->-r requirements.txt (line 5)) (2.6.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.3)\n",
      "Using cached scikit_learn-1.8.0-cp312-cp312-macosx_12_0_arm64.whl (8.1 MB)\n",
      "Using cached scipy-1.17.0-cp312-cp312-macosx_14_0_arm64.whl (20.1 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed scikit-learn-1.8.0 scipy-1.17.0 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dac5e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tokenizers import Tokenizer\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7f3a425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreeyansarora/Downloads/Polar_Representation_Attention/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': 'Resumption of the session', 'fr': 'Reprise de la session'}}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('wmt14', 'fr-en')\n",
    "\n",
    "train_data = dataset['train'].select(range(1000000))\n",
    "test_data = dataset['test']\n",
    "print(train_data[0])\n",
    "\n",
    "def extract_text(dataset, src=\"en\", tgt=\"fr\"):\n",
    "    for example in dataset:\n",
    "        yield example[\"translation\"][src]\n",
    "        yield example[\"translation\"][tgt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d21ef9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=32000,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
    ")\n",
    "\n",
    "tokenizer.train_from_iterator(\n",
    "    extract_text(train_data),\n",
    "    trainer=trainer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "252fdda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ID = tokenizer.token_to_id(\"[PAD]\")\n",
    "BOS_ID = tokenizer.token_to_id(\"[BOS]\")\n",
    "EOS_ID = tokenizer.token_to_id(\"[EOS]\")\n",
    "\n",
    "def tokenize_sentence(sentence, add_special_tokens=True):\n",
    "    encoding = tokenizer.encode(sentence)\n",
    "    token_ids = encoding.ids\n",
    "\n",
    "    if add_special_tokens:\n",
    "        token_ids = [BOS_ID] + token_ids + [EOS_ID]\n",
    "\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa8d5eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import numpy as np\n",
    "\n",
    "def decode_to_tokens(ids_list, tokenizer):\n",
    "    \"\"\"\n",
    "    Converts a list of token IDs into a list of string tokens,\n",
    "    skipping special tokens like [PAD], [BOS], [EOS].\n",
    "    \"\"\"\n",
    "    # .decode() handles the skipping of special tokens automatically\n",
    "    # .split() turns the sentence string into a list of tokens for BLEU\n",
    "    decoded_text = tokenizer.decode(ids_list, skip_special_tokens=True)\n",
    "    return decoded_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06b52fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: list of dicts from wmt14 \n",
    "    Example item: {'translation': {'en': 'Hello', 'fr': 'Bonjour'}}\n",
    "    \"\"\"\n",
    "    src_list = []\n",
    "    tgt_list = []\n",
    "    \n",
    "    for item in batch:\n",
    "        en_text = item['translation']['en']\n",
    "        fr_text = item['translation']['fr']\n",
    "        \n",
    "        src_ids = tokenize_sentence(en_text)[:128] \n",
    "        tgt_ids = tokenize_sentence(fr_text)[:128]\n",
    "        \n",
    "        src_list.append(torch.tensor(src_ids, dtype=torch.long))\n",
    "        tgt_list.append(torch.tensor(tgt_ids, dtype=torch.long))\n",
    "\n",
    "    src_padded = pad_sequence(\n",
    "        src_list,\n",
    "        batch_first=True,\n",
    "        padding_value=PAD_ID\n",
    "    )\n",
    "\n",
    "    tgt_padded = pad_sequence(\n",
    "        tgt_list,\n",
    "        batch_first=True,\n",
    "        padding_value=PAD_ID\n",
    "    )\n",
    "\n",
    "    return src_padded, tgt_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9226d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ee836f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ae0e846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def generate_compact_dataset(num_samples=10000):\n",
    "#    data_tokens = torch.arange(1, 11) \n",
    "#    \n",
    "#    all_inputs = []\n",
    "#    all_targets = []\n",
    "#\n",
    "#    for _ in range(num_samples):\n",
    "#        perm = torch.randperm(10)\n",
    "#        sample_data = data_tokens[perm[:6]]\n",
    "#        \n",
    "#        is_relational = torch.rand(1) > 0.5\n",
    "#        \n",
    "#        if is_relational:\n",
    "#            cmd = torch.tensor([12])\n",
    "#            # Pick a key from the first 5 (so there is a neighbor at +1)\n",
    "#            key_idx = torch.randint(0, 5, (1,)).item()\n",
    "#            query = sample_data[key_idx].view(1)\n",
    "#            target = sample_data[key_idx + 1]\n",
    "#        else:\n",
    "#            # POSITIONAL: Input[7] is an Index (1-6); Target is data at that index\n",
    "#            cmd = torch.tensor([11])\n",
    "#            idx_to_pull = torch.randint(0, 6, (1,)).item()\n",
    "#            query = torch.tensor([idx_to_pull + 1])\n",
    "#            target = sample_data[idx_to_pull]\n",
    "#\n",
    "#        full_input = torch.cat([sample_data, cmd, query])\n",
    "#        \n",
    "#        all_inputs.append(full_input)\n",
    "#        all_targets.append(target)\n",
    "#\n",
    "#    return torch.stack(all_inputs), torch.stack(all_targets)\n",
    "#\n",
    "## Generate the 10,000 samples\n",
    "#inputs, targets = generate_compact_dataset(10000)\n",
    "#\n",
    "#print(f\"Dataset Shape: {inputs.shape}\") # [10000, 8]\n",
    "#print(f\"Sample 0 (Input): {inputs[0].tolist()} -> Target: {targets[0].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd4f18a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pe(seq_len, d_model):\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    position = torch.arange(0, seq_len).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(1000000.0) / d_model))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "934908a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class embeddings(nn.Module):\n",
    "    def __init__(self, d, vocab_size=32000, max_len=512):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d)\n",
    "        self.register_buffer('pe', get_pe(max_len, d))\n",
    "\n",
    "    def forward(self, x):\n",
    "        te = self.token_emb(x)\n",
    "        return te + self.pe[:, :x.size(1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bab1d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices = torch.randperm(len(inputs))\n",
    "#\n",
    "#train_size = int(0.5*len(inputs))\n",
    "#\n",
    "#train_idx = indices[:train_size]\n",
    "#test_idx = indices[train_size:]\n",
    "#\n",
    "#train_inputs, train_targets = inputs[train_idx], targets[train_idx]\n",
    "#test_inputs,  test_targets  = inputs[test_idx],  targets[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71b41dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, d_k=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.query_v1 = nn.Linear(d_k, d_k, bias=False)\n",
    "        self.key_v1 = nn.Linear(d_k, d_k, bias=False)\n",
    "        self.value_v1 = nn.Linear(d_k, d_k, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        Q1 = self.query_v1(x)\n",
    "        K1 = self.key_v1(x)\n",
    "        V1 = self.value_v1(x)\n",
    "        att1 = torch.matmul(Q1, K1.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            att1 = att1.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        att_soft1 = torch.softmax(att1, dim=-1)\n",
    "        att_soft1 = self.dropout(att_soft1)\n",
    "\n",
    "        out1 = torch.matmul(att_soft1, V1)\n",
    "        return out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fdff8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelArchitecture(nn.Module):\n",
    "    def __init__(self, vocab_size=32000, d_k=512, n_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = embeddings(512)\n",
    "        self.attention = AttentionModule(d_k, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_k)\n",
    "        self.norm2 = nn.LayerNorm(d_k)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_k, n_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(n_ff, d_k)\n",
    "        )\n",
    "\n",
    "        self.unembed = nn.Linear(d_k, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x_emb = self.embedding(x) * math.sqrt(512)\n",
    "        att_out = self.attention(x_emb, mask)\n",
    "        x = self.norm1(x_emb + self.dropout(att_out))\n",
    "        mlp_out = self.mlp(x)\n",
    "        x = self.norm2(x + self.dropout(mlp_out))\n",
    "        \n",
    "        logits = self.unembed(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db621054",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = ModelArchitecture().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1, ignore_index=PAD_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be42e25",
   "metadata": {},
   "source": [
    "# Test without scheduled LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64077c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def noam_lambda(step, d_model=512, warmup=4000):\n",
    "#    # The scheduler passes the current_step (starting at 0)\n",
    "#    # Adding 1 prevents division by zero\n",
    "#    step += 1 \n",
    "#    return (d_model ** -0.5) * min(step ** -0.5, step * (warmup ** -1.5))\n",
    "#\n",
    "## Use 1.0 as the base LR so the lambda controls the absolute value\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.98), eps=1e-9, weight_decay = 0.01)\n",
    "#scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=noam_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fbbb41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Step:    1000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.6793 | Train Loss: 3.6288 |\n",
      "| Test Acc:  0.5919 | Test Loss:  4.6960 |\n",
      "| Test F1:   0.5121 | Test BLEU:  0.2429 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:    2000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.6858 | Train Loss: 3.6420 |\n",
      "| Test Acc:  0.5904 | Test Loss:  4.7003 |\n",
      "| Test F1:   0.5265 | Test BLEU:  0.2416 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:    3000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.6955 | Train Loss: 3.5681 |\n",
      "| Test Acc:  0.5952 | Test Loss:  4.6223 |\n",
      "| Test F1:   0.5411 | Test BLEU:  0.1208 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:    4000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7446 | Train Loss: 3.3325 |\n",
      "| Test Acc:  0.5976 | Test Loss:  4.6008 |\n",
      "| Test F1:   0.5446 | Test BLEU:  0.1308 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:    5000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7166 | Train Loss: 3.4341 |\n",
      "| Test Acc:  0.6235 | Test Loss:  4.4127 |\n",
      "| Test F1:   0.5621 | Test BLEU:  0.1395 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:    6000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7040 | Train Loss: 3.5705 |\n",
      "| Test Acc:  0.6143 | Test Loss:  4.4363 |\n",
      "| Test F1:   0.5481 | Test BLEU:  0.1319 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:    7000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.6995 | Train Loss: 3.5208 |\n",
      "| Test Acc:  0.6236 | Test Loss:  4.3623 |\n",
      "| Test F1:   0.5567 | Test BLEU:  0.1402 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:    8000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7126 | Train Loss: 3.4846 |\n",
      "| Test Acc:  0.6205 | Test Loss:  4.3781 |\n",
      "| Test F1:   0.5499 | Test BLEU:  0.1398 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:    9000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7216 | Train Loss: 3.5434 |\n",
      "| Test Acc:  0.6396 | Test Loss:  4.2580 |\n",
      "| Test F1:   0.5590 | Test BLEU:  0.1508 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   10000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7157 | Train Loss: 3.4246 |\n",
      "| Test Acc:  0.6457 | Test Loss:  4.2276 |\n",
      "| Test F1:   0.5663 | Test BLEU:  0.1555 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   11000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7333 | Train Loss: 3.2754 |\n",
      "| Test Acc:  0.6468 | Test Loss:  4.2289 |\n",
      "| Test F1:   0.5674 | Test BLEU:  0.1569 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   12000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7071 | Train Loss: 3.5432 |\n",
      "| Test Acc:  0.6502 | Test Loss:  4.1981 |\n",
      "| Test F1:   0.5627 | Test BLEU:  0.1596 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   13000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7429 | Train Loss: 3.3625 |\n",
      "| Test Acc:  0.6384 | Test Loss:  4.2597 |\n",
      "| Test F1:   0.5633 | Test BLEU:  0.1508 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   14000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.6588 | Train Loss: 3.7434 |\n",
      "| Test Acc:  0.6439 | Test Loss:  4.2319 |\n",
      "| Test F1:   0.5556 | Test BLEU:  0.1549 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   15000 | Epoch:   0 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7603 | Train Loss: 3.1576 |\n",
      "| Test Acc:  0.6451 | Test Loss:  4.2248 |\n",
      "| Test F1:   0.5671 | Test BLEU:  0.1545 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   16000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.6742 | Train Loss: 3.6123 |\n",
      "| Test Acc:  0.5673 | Test Loss:  4.7341 |\n",
      "| Test F1:   0.4799 | Test BLEU:  0.1136 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   17000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7942 | Train Loss: 2.9027 |\n",
      "| Test Acc:  0.6317 | Test Loss:  4.3047 |\n",
      "| Test F1:   0.5613 | Test BLEU:  0.1462 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   18000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.6907 | Train Loss: 3.6864 |\n",
      "| Test Acc:  0.6458 | Test Loss:  4.2167 |\n",
      "| Test F1:   0.5610 | Test BLEU:  0.1559 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   19000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7344 | Train Loss: 3.3469 |\n",
      "| Test Acc:  0.6480 | Test Loss:  4.2046 |\n",
      "| Test F1:   0.5682 | Test BLEU:  0.1561 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   20000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7055 | Train Loss: 3.5576 |\n",
      "| Test Acc:  0.6517 | Test Loss:  4.1874 |\n",
      "| Test F1:   0.5663 | Test BLEU:  0.1590 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   21000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7291 | Train Loss: 3.3105 |\n",
      "| Test Acc:  0.6493 | Test Loss:  4.1970 |\n",
      "| Test F1:   0.5610 | Test BLEU:  0.1576 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   22000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.6740 | Train Loss: 3.5818 |\n",
      "| Test Acc:  0.6206 | Test Loss:  4.3738 |\n",
      "| Test F1:   0.5368 | Test BLEU:  0.1387 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   23000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7353 | Train Loss: 3.3673 |\n",
      "| Test Acc:  0.6507 | Test Loss:  4.1823 |\n",
      "| Test F1:   0.5706 | Test BLEU:  0.1594 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   24000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7534 | Train Loss: 3.2860 |\n",
      "| Test Acc:  0.6283 | Test Loss:  4.2933 |\n",
      "| Test F1:   0.5524 | Test BLEU:  0.1446 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   25000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.6742 | Train Loss: 3.6434 |\n",
      "| Test Acc:  0.6218 | Test Loss:  4.3224 |\n",
      "| Test F1:   0.5482 | Test BLEU:  0.1410 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   26000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7505 | Train Loss: 3.2500 |\n",
      "| Test Acc:  0.5799 | Test Loss:  4.5738 |\n",
      "| Test F1:   0.5097 | Test BLEU:  0.1121 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   27000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7774 | Train Loss: 3.0952 |\n",
      "| Test Acc:  0.6015 | Test Loss:  4.4394 |\n",
      "| Test F1:   0.5206 | Test BLEU:  0.1272 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   28000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7365 | Train Loss: 3.4149 |\n",
      "| Test Acc:  0.6084 | Test Loss:  4.3979 |\n",
      "| Test F1:   0.5304 | Test BLEU:  0.1319 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   29000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7619 | Train Loss: 3.1398 |\n",
      "| Test Acc:  0.5644 | Test Loss:  4.6287 |\n",
      "| Test F1:   0.4891 | Test BLEU:  0.1035 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   30000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7733 | Train Loss: 3.1766 |\n",
      "| Test Acc:  0.5752 | Test Loss:  4.5698 |\n",
      "| Test F1:   0.4980 | Test BLEU:  0.1096 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   31000 | Epoch:   1 | LR: 0.00030000 |\n",
      "| Train Acc: 0.6776 | Train Loss: 3.7868 |\n",
      "| Test Acc:  0.6312 | Test Loss:  4.2771 |\n",
      "| Test F1:   0.5464 | Test BLEU:  0.1474 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   32000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7553 | Train Loss: 3.1382 |\n",
      "| Test Acc:  0.5957 | Test Loss:  4.4555 |\n",
      "| Test F1:   0.5160 | Test BLEU:  0.1226 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   33000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7797 | Train Loss: 3.0832 |\n",
      "| Test Acc:  0.6005 | Test Loss:  4.4533 |\n",
      "| Test F1:   0.5232 | Test BLEU:  0.1250 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   34000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7058 | Train Loss: 3.4719 |\n",
      "| Test Acc:  0.5774 | Test Loss:  4.5588 |\n",
      "| Test F1:   0.4945 | Test BLEU:  0.1112 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   35000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7397 | Train Loss: 3.2976 |\n",
      "| Test Acc:  0.5647 | Test Loss:  4.6300 |\n",
      "| Test F1:   0.4815 | Test BLEU:  0.1026 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   36000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7002 | Train Loss: 3.5582 |\n",
      "| Test Acc:  0.5757 | Test Loss:  4.5595 |\n",
      "| Test F1:   0.4925 | Test BLEU:  0.1085 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   37000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7339 | Train Loss: 3.3677 |\n",
      "| Test Acc:  0.5588 | Test Loss:  4.5986 |\n",
      "| Test F1:   0.4731 | Test BLEU:  0.0994 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   38000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7155 | Train Loss: 3.5082 |\n",
      "| Test Acc:  0.5913 | Test Loss:  4.4714 |\n",
      "| Test F1:   0.5113 | Test BLEU:  0.1195 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   39000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7405 | Train Loss: 3.2730 |\n",
      "| Test Acc:  0.5996 | Test Loss:  4.4315 |\n",
      "| Test F1:   0.5181 | Test BLEU:  0.1254 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   40000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7298 | Train Loss: 3.3084 |\n",
      "| Test Acc:  0.5502 | Test Loss:  4.7009 |\n",
      "| Test F1:   0.4673 | Test BLEU:  0.0921 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   41000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7526 | Train Loss: 3.1519 |\n",
      "| Test Acc:  0.6001 | Test Loss:  4.4369 |\n",
      "| Test F1:   0.5214 | Test BLEU:  0.1252 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   42000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7407 | Train Loss: 3.2660 |\n",
      "| Test Acc:  0.5839 | Test Loss:  4.5103 |\n",
      "| Test F1:   0.5015 | Test BLEU:  0.1150 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   43000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7292 | Train Loss: 3.4205 |\n",
      "| Test Acc:  0.5589 | Test Loss:  4.6072 |\n",
      "| Test F1:   0.4776 | Test BLEU:  0.0988 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   44000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7706 | Train Loss: 3.0653 |\n",
      "| Test Acc:  0.5966 | Test Loss:  4.4396 |\n",
      "| Test F1:   0.5154 | Test BLEU:  0.1236 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   45000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7332 | Train Loss: 3.1703 |\n",
      "| Test Acc:  0.6295 | Test Loss:  4.2795 |\n",
      "| Test F1:   0.5385 | Test BLEU:  0.1452 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   46000 | Epoch:   2 | LR: 0.00030000 |\n",
      "| Train Acc: 0.6966 | Train Loss: 3.5089 |\n",
      "| Test Acc:  0.6124 | Test Loss:  4.3546 |\n",
      "| Test F1:   0.5304 | Test BLEU:  0.1337 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   47000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7458 | Train Loss: 3.2072 |\n",
      "| Test Acc:  0.5867 | Test Loss:  4.5003 |\n",
      "| Test F1:   0.5029 | Test BLEU:  0.1163 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   48000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.6033 | Train Loss: 4.1609 |\n",
      "| Test Acc:  0.5443 | Test Loss:  4.8281 |\n",
      "| Test F1:   0.4730 | Test BLEU:  0.1272 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   49000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7642 | Train Loss: 3.0952 |\n",
      "| Test Acc:  0.5956 | Test Loss:  4.4364 |\n",
      "| Test F1:   0.5129 | Test BLEU:  0.1221 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   50000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7235 | Train Loss: 3.3393 |\n",
      "| Test Acc:  0.6051 | Test Loss:  4.3703 |\n",
      "| Test F1:   0.5239 | Test BLEU:  0.1287 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   51000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7432 | Train Loss: 3.2994 |\n",
      "| Test Acc:  0.5835 | Test Loss:  4.4778 |\n",
      "| Test F1:   0.5001 | Test BLEU:  0.1150 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   52000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7444 | Train Loss: 3.2795 |\n",
      "| Test Acc:  0.5841 | Test Loss:  4.4851 |\n",
      "| Test F1:   0.5052 | Test BLEU:  0.1140 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   53000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7232 | Train Loss: 3.5018 |\n",
      "| Test Acc:  0.5850 | Test Loss:  4.5254 |\n",
      "| Test F1:   0.5057 | Test BLEU:  0.1158 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   54000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7461 | Train Loss: 3.1770 |\n",
      "| Test Acc:  0.6173 | Test Loss:  4.3288 |\n",
      "| Test F1:   0.5339 | Test BLEU:  0.1369 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   55000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7819 | Train Loss: 3.0605 |\n",
      "| Test Acc:  0.5926 | Test Loss:  4.4338 |\n",
      "| Test F1:   0.5053 | Test BLEU:  0.1208 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   56000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7535 | Train Loss: 3.1796 |\n",
      "| Test Acc:  0.6199 | Test Loss:  4.3309 |\n",
      "| Test F1:   0.5387 | Test BLEU:  0.1373 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   57000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7603 | Train Loss: 3.2125 |\n",
      "| Test Acc:  0.5511 | Test Loss:  4.6755 |\n",
      "| Test F1:   0.4680 | Test BLEU:  0.0938 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   58000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.6910 | Train Loss: 3.7097 |\n",
      "| Test Acc:  0.5765 | Test Loss:  4.5283 |\n",
      "| Test F1:   0.4917 | Test BLEU:  0.1092 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   59000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7643 | Train Loss: 3.0974 |\n",
      "| Test Acc:  0.5521 | Test Loss:  4.6048 |\n",
      "| Test F1:   0.4682 | Test BLEU:  0.0984 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   60000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7471 | Train Loss: 3.1876 |\n",
      "| Test Acc:  0.5706 | Test Loss:  4.5268 |\n",
      "| Test F1:   0.4847 | Test BLEU:  0.1086 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   61000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7108 | Train Loss: 3.4842 |\n",
      "| Test Acc:  0.5896 | Test Loss:  4.4495 |\n",
      "| Test F1:   0.5101 | Test BLEU:  0.1183 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   62000 | Epoch:   3 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7783 | Train Loss: 3.0206 |\n",
      "| Test Acc:  0.6056 | Test Loss:  4.3759 |\n",
      "| Test F1:   0.5216 | Test BLEU:  0.1284 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   63000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7401 | Train Loss: 3.3364 |\n",
      "| Test Acc:  0.6173 | Test Loss:  4.3256 |\n",
      "| Test F1:   0.5376 | Test BLEU:  0.1361 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   64000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7500 | Train Loss: 3.2705 |\n",
      "| Test Acc:  0.5938 | Test Loss:  4.4249 |\n",
      "| Test F1:   0.5130 | Test BLEU:  0.1209 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   65000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7262 | Train Loss: 3.4069 |\n",
      "| Test Acc:  0.5082 | Test Loss:  4.8709 |\n",
      "| Test F1:   0.4231 | Test BLEU:  0.0698 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   66000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7175 | Train Loss: 3.4578 |\n",
      "| Test Acc:  0.5356 | Test Loss:  4.7010 |\n",
      "| Test F1:   0.4514 | Test BLEU:  0.0836 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   67000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7479 | Train Loss: 3.1720 |\n",
      "| Test Acc:  0.5627 | Test Loss:  4.5692 |\n",
      "| Test F1:   0.4768 | Test BLEU:  0.1010 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   68000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7582 | Train Loss: 3.1452 |\n",
      "| Test Acc:  0.6209 | Test Loss:  4.3104 |\n",
      "| Test F1:   0.5301 | Test BLEU:  0.1395 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   69000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.8021 | Train Loss: 2.8837 |\n",
      "| Test Acc:  0.5648 | Test Loss:  4.5419 |\n",
      "| Test F1:   0.4775 | Test BLEU:  0.1029 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   70000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7556 | Train Loss: 3.2178 |\n",
      "| Test Acc:  0.5421 | Test Loss:  4.6294 |\n",
      "| Test F1:   0.4560 | Test BLEU:  0.0886 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   71000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7505 | Train Loss: 3.2123 |\n",
      "| Test Acc:  0.5550 | Test Loss:  4.5977 |\n",
      "| Test F1:   0.4670 | Test BLEU:  0.0961 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   72000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7472 | Train Loss: 3.0923 |\n",
      "| Test Acc:  0.5927 | Test Loss:  4.4283 |\n",
      "| Test F1:   0.5042 | Test BLEU:  0.1212 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   73000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.6819 | Train Loss: 3.6137 |\n",
      "| Test Acc:  0.5437 | Test Loss:  4.8468 |\n",
      "| Test F1:   0.4747 | Test BLEU:  0.1118 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   74000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7484 | Train Loss: 3.1811 |\n",
      "| Test Acc:  0.5822 | Test Loss:  4.4796 |\n",
      "| Test F1:   0.4966 | Test BLEU:  0.1134 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   75000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7193 | Train Loss: 3.4798 |\n",
      "| Test Acc:  0.5960 | Test Loss:  4.4132 |\n",
      "| Test F1:   0.5159 | Test BLEU:  0.1210 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   76000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7495 | Train Loss: 3.2311 |\n",
      "| Test Acc:  0.5881 | Test Loss:  4.4604 |\n",
      "| Test F1:   0.5052 | Test BLEU:  0.1173 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   77000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7285 | Train Loss: 3.2606 |\n",
      "| Test Acc:  0.5647 | Test Loss:  4.5971 |\n",
      "| Test F1:   0.4820 | Test BLEU:  0.1021 |\n",
      "---------------------------------------------------------------------------\n",
      "| Step:   78000 | Epoch:   4 | LR: 0.00030000 |\n",
      "| Train Acc: 0.7273 | Train Loss: 3.3360 |\n",
      "| Test Acc:  0.6115 | Test Loss:  4.3549 |\n",
      "| Test F1:   0.5286 | Test BLEU:  0.1319 |\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.to(DEVICE)\n",
    "global_step = 0\n",
    "log_interval = 1000\n",
    "accumulation_steps = 4 \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad() \n",
    "    \n",
    "    for i, (src, tgt) in enumerate(train_loader):\n",
    "        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "        \n",
    "        decoder_input  = tgt[:, :-1]  \n",
    "        decoder_target = tgt[:, 1:] \n",
    "        logits = model(decoder_input) \n",
    "\n",
    "        # 1. Standard Loss Calculation\n",
    "        loss = criterion(\n",
    "            logits.reshape(-1, logits.size(-1)),\n",
    "            decoder_target.reshape(-1)\n",
    "        )\n",
    "        \n",
    "        # Scale loss so gradients are averaged across the full effective batch\n",
    "        loss_scaled = loss / accumulation_steps \n",
    "        loss_scaled.backward()\n",
    "\n",
    "        # Train Accuracy (for the current mini-batch)\n",
    "        with torch.no_grad():\n",
    "            pred_tokens = logits.argmax(-1)\n",
    "            mask = decoder_target != PAD_ID\n",
    "            train_acc = ((pred_tokens == decoder_target) & mask).float().sum() / mask.sum()\n",
    "\n",
    "        # 2. Only update weights and LOG every 'accumulation_steps'\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            # scheduler.step() # Uncomment if using Noam\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            # --- EVALUATION BLOCK: Now strictly inside the update check ---\n",
    "            if global_step % log_interval == 0:\n",
    "                model.eval()\n",
    "                total_test_loss = 0\n",
    "                all_preds_ids, all_trues_ids = [], []\n",
    "                bleu_hypotheses, bleu_references = [], []\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for test_src, test_tgt in test_loader:\n",
    "                        test_src, test_tgt = test_src.to(DEVICE), test_tgt.to(DEVICE)\n",
    "                        dec_in_test, dec_tgt_test = test_tgt[:, :-1], test_tgt[:, 1:]\n",
    "                        \n",
    "                        test_logits = model(dec_in_test)\n",
    "                        t_loss = criterion(test_logits.reshape(-1, test_logits.size(-1)), dec_tgt_test.reshape(-1))\n",
    "                        total_test_loss += t_loss.item()\n",
    "\n",
    "                        test_pred_ids = test_logits.argmax(-1)\n",
    "                        test_mask = (dec_tgt_test != PAD_ID)\n",
    "                        \n",
    "                        all_preds_ids.extend(test_pred_ids[test_mask].cpu().numpy())\n",
    "                        all_trues_ids.extend(dec_tgt_test[test_mask].cpu().numpy())\n",
    "\n",
    "                        for b in range(test_pred_ids.size(0)):\n",
    "                            hyp_tokens = decode_to_tokens(test_pred_ids[b].tolist(), tokenizer)\n",
    "                            ref_tokens = decode_to_tokens(dec_tgt_test[b].tolist(), tokenizer)\n",
    "                            bleu_hypotheses.append(hyp_tokens)\n",
    "                            bleu_references.append([ref_tokens])\n",
    "\n",
    "                # Metric Calculations\n",
    "                avg_test_loss = total_test_loss / len(test_loader)\n",
    "                test_acc = (np.array(all_preds_ids) == np.array(all_trues_ids)).mean()\n",
    "                test_f1 = f1_score(all_trues_ids, all_preds_ids, average='weighted', zero_division=0)\n",
    "                test_bleu = corpus_bleu(bleu_references, bleu_hypotheses)\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                \n",
    "                output_str = (\n",
    "                    f\"| Step: {global_step:7d} | Epoch: {epoch:3d} | LR: {current_lr:.8f} |\\n\"\n",
    "                    f\"| Train Acc: {train_acc.item():.4f} | Train Loss: {loss.item():.4f} |\\n\"\n",
    "                    f\"| Test Acc:  {test_acc:.4f} | Test Loss:  {avg_test_loss:.4f} |\\n\"\n",
    "                    f\"| Test F1:   {test_f1:.4f} | Test BLEU:  {test_bleu:.4f} |\\n\"\n",
    "                    f\"{'-'*75}\"\n",
    "                )\n",
    "                print(output_str)\n",
    "                with open(\"step_results_logging.log\", \"a\") as f:\n",
    "                    f.write(output_str + \"\\n\")\n",
    "                \n",
    "                model.train() # Switch back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ebf128",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
